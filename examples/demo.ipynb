{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305d91e1",
   "metadata": {},
   "source": [
    "# Framework Demo: Computational Graph & XOR Training\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. How the computational graph works\n",
    "2. Training a neural network on the XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6f24e",
   "metadata": {},
   "source": [
    "## 1. Computational Graph Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6507d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂L/∂W = [[2.2 4.4]]\n",
      "∂L/∂x = [[1.1  0.66]]\n"
     ]
    }
   ],
   "source": [
    "from minigrad.tensor import Tensor\n",
    "\n",
    "# Create tensors\n",
    "x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "W = Tensor([[0.5], [0.3]], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x.matmul(W)\n",
    "loss = (y * y).sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(\"∂L/∂W =\", W.grad.T)\n",
    "print(\"∂L/∂x =\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16455ca",
   "metadata": {},
   "source": [
    "## 2. XOR Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a9550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minigrad.nn.layers import Linear\n",
    "from minigrad.nn.activations import ReLU, Sigmoid\n",
    "from minigrad.nn.sequential import Sequential\n",
    "from minigrad.losses.losses import MSELoss\n",
    "from minigrad.data.data import TensorDataset, DataLoader\n",
    "from minigrad.optimizers.optimizers import SGD\n",
    "from minigrad.train.train import Trainer, TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e7bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential(Linear(2, 8), ReLU(), Linear(8, 1), Sigmoid())\n",
    "\n",
    "# Prepare data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=float)\n",
    "ds = TensorDataset(X, y)\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# Setup training\n",
    "loss_fn = MSELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "trainer = Trainer(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734177c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1  train=1.175553\n",
      "Epoch   20  train=0.023442\n",
      "Epoch   40  train=0.000438\n",
      "Epoch   60  train=0.000178\n",
      "Epoch   80  train=0.000139\n",
      "Epoch  100  train=0.000128\n",
      "Epoch  120  train=0.000121\n",
      "Epoch  140  train=0.000116\n",
      "Epoch  160  train=0.000111\n",
      "Epoch  180  train=0.000106\n",
      "Epoch  200  train=0.000103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1755530612211693,\n",
       " 1.1001988732938757,\n",
       " 0.9841631201636728,\n",
       " 0.8819582160814281,\n",
       " 0.8399157353214425,\n",
       " 0.7852917412903311,\n",
       " 0.7177164600706128,\n",
       " 0.6452324464505192,\n",
       " 0.5381562439487849,\n",
       " 0.43000035178592827,\n",
       " 0.3641403309280292,\n",
       " 0.291457744102423,\n",
       " 0.22253032653731303,\n",
       " 0.15604931542970643,\n",
       " 0.10670146623849251,\n",
       " 0.07950706737554547,\n",
       " 0.05950139703251876,\n",
       " 0.04438799365788427,\n",
       " 0.03304521370988822,\n",
       " 0.02344163552772676,\n",
       " 0.016057851454730297,\n",
       " 0.010807433916726554,\n",
       " 0.007360072567103779,\n",
       " 0.005271419499080426,\n",
       " 0.003860553264451291,\n",
       " 0.0028984463847834627,\n",
       " 0.0022331404826018996,\n",
       " 0.0017661793032087017,\n",
       " 0.0014891673972774332,\n",
       " 0.00127385007826233,\n",
       " 0.0011035061598399494,\n",
       " 0.0009665450587566297,\n",
       " 0.0008547890065625986,\n",
       " 0.0007623779302184579,\n",
       " 0.0006850522116444682,\n",
       " 0.0006196722702566356,\n",
       " 0.0005638911909118735,\n",
       " 0.000515928829601753,\n",
       " 0.0004744145155395856,\n",
       " 0.00043827673860221935,\n",
       " 0.0004066652890556649,\n",
       " 0.0003788959254388798,\n",
       " 0.000354410733336353,\n",
       " 0.0003327494447033754,\n",
       " 0.00031352844088615027,\n",
       " 0.00029642516952302854,\n",
       " 0.0002811664033506965,\n",
       " 0.0002675192514618906,\n",
       " 0.00025528416619444443,\n",
       " 0.00024428941749934225,\n",
       " 0.00023438666356533343,\n",
       " 0.0002254473541789737,\n",
       " 0.0002173597773813875,\n",
       " 0.00021002661119167657,\n",
       " 0.00020336287783951273,\n",
       " 0.00019729422306853,\n",
       " 0.00019175546100047648,\n",
       " 0.00018668933805177066,\n",
       " 0.00018204547899061059,\n",
       " 0.00017777948543819873,\n",
       " 0.00017385216264263664,\n",
       " 0.0001702288546597809,\n",
       " 0.00016687887148356638,\n",
       " 0.00016377499440407633,\n",
       " 0.00016089304809325624,\n",
       " 0.00015821152973975104,\n",
       " 0.000155711287059586,\n",
       " 0.00015337523826119422,\n",
       " 0.00015118812808891314,\n",
       " 0.0001491363149467213,\n",
       " 0.0001478857862437863,\n",
       " 0.00014673196932464872,\n",
       " 0.00014563966086043206,\n",
       " 0.00014460359049045968,\n",
       " 0.00014361902927456156,\n",
       " 0.00014268172409366613,\n",
       " 0.00014178784119195254,\n",
       " 0.0001409339174718068,\n",
       " 0.00014011681836929168,\n",
       " 0.00013933370132187544,\n",
       " 0.0001385819839955513,\n",
       " 0.00013785931656983206,\n",
       " 0.00013716355748958291,\n",
       " 0.00013649275218566143,\n",
       " 0.00013584511434420858,\n",
       " 0.00013521900936999052,\n",
       " 0.000134612939743915,\n",
       " 0.00013402553202080547,\n",
       " 0.000133455525252029,\n",
       " 0.00013290176064970958,\n",
       " 0.00013236317233641286,\n",
       " 0.00013183877904678434,\n",
       " 0.00013132767666674817,\n",
       " 0.00013082903151180958,\n",
       " 0.0001303420742596963,\n",
       " 0.00012986609446385982,\n",
       " 0.0001294004355840171,\n",
       " 0.00012894449047830866,\n",
       " 0.0001284976973083947,\n",
       " 0.00012805953581498047,\n",
       " 0.00012762952392625203,\n",
       " 0.00012720721466616598,\n",
       " 0.00012679219333330767,\n",
       " 0.00012638407492428488,\n",
       " 0.00012598250177853746,\n",
       " 0.00012558714142383562,\n",
       " 0.0001251976846039958,\n",
       " 0.00012481384347221895,\n",
       " 0.00012443534993509492,\n",
       " 0.00012406195413383644,\n",
       " 0.00012369342305064,\n",
       " 0.000123354772375471,\n",
       " 0.00012311736475252756,\n",
       " 0.00012283177671550447,\n",
       " 0.00012250304414202645,\n",
       " 0.0001221359528297876,\n",
       " 0.00012180664346344609,\n",
       " 0.0001215311776740564,\n",
       " 0.00012125214497913828,\n",
       " 0.00012097009981903914,\n",
       " 0.00012068554336204577,\n",
       " 0.00012039892763735705,\n",
       " 0.00012011065949631337,\n",
       " 0.00011982110437633844,\n",
       " 0.00011953058985165553,\n",
       " 0.00011923940896247948,\n",
       " 0.00011894782332016228,\n",
       " 0.00011865606599036498,\n",
       " 0.00011843415063913432,\n",
       " 0.00011818327213312748,\n",
       " 0.00011789036592670734,\n",
       " 0.00011758564691754093,\n",
       " 0.00011733641841897972,\n",
       " 0.00011708360173286788,\n",
       " 0.00011682770894777064,\n",
       " 0.00011656920329495269,\n",
       " 0.0001163085031157823,\n",
       " 0.00011604598561025199,\n",
       " 0.00011578199035744762,\n",
       " 0.00011553735758125801,\n",
       " 0.00011528646993044623,\n",
       " 0.00011503125863836672,\n",
       " 0.00011479176819401393,\n",
       " 0.00011454911138211572,\n",
       " 0.00011430374180743622,\n",
       " 0.00011405606955285411,\n",
       " 0.00011380646481894974,\n",
       " 0.00011358220096163706,\n",
       " 0.00011333587099390737,\n",
       " 0.00011309520633252407,\n",
       " 0.00011286784050337435,\n",
       " 0.00011263706491114426,\n",
       " 0.00011240334311832217,\n",
       " 0.00011216709476992713,\n",
       " 0.0001119286992274527,\n",
       " 0.00011168849898129966,\n",
       " 0.00011148490943408216,\n",
       " 0.00011125477782099566,\n",
       " 0.00011100463098700673,\n",
       " 0.00011078616544502448,\n",
       " 0.00011056440321773887,\n",
       " 0.00011033978618813203,\n",
       " 0.00011011271444494883,\n",
       " 0.00010988354976401282,\n",
       " 0.00010968915787938098,\n",
       " 0.00010946235102754774,\n",
       " 0.00010923010623810968,\n",
       " 0.00010902104132820389,\n",
       " 0.0001088085454064037,\n",
       " 0.00010859306445634558,\n",
       " 0.00010837500259062869,\n",
       " 0.00010815472551414844,\n",
       " 0.00010793256377347296,\n",
       " 0.00010775653155133191,\n",
       " 0.00010754404252040522,\n",
       " 0.00010729987276990769,\n",
       " 0.00010709766074167353,\n",
       " 0.00010689218585334081,\n",
       " 0.00010668387078675094,\n",
       " 0.00010647309848309426,\n",
       " 0.0001062697018595246,\n",
       " 0.00010605983478438435,\n",
       " 0.0001058564596654856,\n",
       " 0.00010565216050858864,\n",
       " 0.00010545643780891816,\n",
       " 0.00010525901057894682,\n",
       " 0.00010505863005858696,\n",
       " 0.00010485568380976626,\n",
       " 0.00010465052273551564,\n",
       " 0.0001044715480799313,\n",
       " 0.00010426627762754942,\n",
       " 0.00010406501290258316,\n",
       " 0.00010387748381206239,\n",
       " 0.00010368657612617942,\n",
       " 0.00010349271198781093,\n",
       " 0.00010329627415011177,\n",
       " 0.00010309760924042983,\n",
       " 0.00010289703081931093,\n",
       " 0.00010272717477756099,\n",
       " 0.00010253575650688794]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.fit(dl, cfg=TrainConfig(epochs=200, print_every=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cdf05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.008]\n",
      " [0.996]\n",
      " [0.996]\n",
      " [0.003]]\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "predictions = model(Tensor(X))\n",
    "print(\"Predictions:\", np.round(predictions.data, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
